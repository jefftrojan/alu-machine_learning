{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Applications.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOU7BD1CO3zTNN9CQ25Hr+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmerchan/holbertonschool-machine_learning/blob/main/Transformer_Applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3TKw87CSJJy",
        "outputId": "32e4262a-ebf7-4798-aa90-eadf29790fc7"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"                                                                                        \n",
        "    Loads and preps a dataset for machine translation                                          \n",
        "                                                                                               \n",
        "    class constructor:                                                                         \n",
        "        def __init__(self)                                                                     \n",
        "                                                                                               \n",
        "    public instance attributes:                                                                \n",
        "        data_train:                                                                            \n",
        "            contains the ted_hrlr_translate/pt_to_en                                           \n",
        "                tf.data.Dataset train split, loaded as_supervided                              \n",
        "        data_valid:                                                                            \n",
        "            contains the ted_hrlr_translate/pt_to_en                                           \n",
        "                tf.data.Dataset validate split, loaded as_supervided                           \n",
        "        tokenizer_pt:                                                                          \n",
        "            the Portuguese tokenizer created from the training set                             \n",
        "        tokenizer_en:                                                                          \n",
        "            the English tokenizer created from the training set                                \n",
        "                                                                                               \n",
        "    instance method:                                                                           \n",
        "        def tokenize_dataset(self, data):                                                      \n",
        "            that creates sub-word tokenizers for our dataset                                   \n",
        "        def encode(self, pt, en):                                                              \n",
        "            that encodes a translation into tokens \n",
        "        def tf_encode(self, pt, en):                                                           \n",
        "            that acts as a TensorFlow wrapper for the encode method                            \n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, max_len):\n",
        "        \"\"\"                                                                                    \n",
        "        Class constructor                                                                      \n",
        "                                                                                               \n",
        "        parameters:                                                                            \n",
        "            batch_size [int]:                                                                  \n",
        "                the batch size for training/validation                                         \n",
        "            max_len [int]:                                                                     \n",
        "                the maximum number of tokens allowed per example sentence                      \n",
        "                                                                                               \n",
        "        Sets the public instance attributes:                                                   \n",
        "            data_train:                                                                        \n",
        "                contains the ted_hrlr_translate/pt_to_en                                       \n",
        "                    tf.data.Dataset train split, loaded as_supervided                          \n",
        "            data_valid:                                                                        \n",
        "                contains the ted_hrlr_translate/pt_to_en                                       \n",
        "                    tf.data.Dataset validate split, loaded as_supervided                       \n",
        "            tokenizer_pt:                                                                      \n",
        "                the Portuguese tokenizer created from the training set                         \n",
        "            tokenizer_en:                                                                      \n",
        "                the English tokenizer created from the training set                            \n",
        "        \"\"\"\n",
        "        data_train = tfds.load(\"ted_hrlr_translate/pt_to_en\",\n",
        "                               split=\"train\",\n",
        "                               as_supervised=True)\n",
        "        data_valid = tfds.load(\"ted_hrlr_translate/pt_to_en\",\n",
        "                               split=\"validation\",\n",
        "                               as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(\n",
        "            data_train)\n",
        "\n",
        "        # set attributes to encoded data                                                       \n",
        "        self.data_train = data_train.map(self.tf_encode)\n",
        "        self.data_valid = data_valid.map(self.tf_encode)\n",
        "\n",
        "        def filter_max_length(x, y, max_len=max_len):\n",
        "            \"\"\"                                                                                \n",
        "            Filters data by max_len                                                            \n",
        "            \"\"\"\n",
        "            filtered = tf.logical_and(tf.size(x) <= max_len,\n",
        "                                      tf.size(y) <= max_len)\n",
        "            return filtered\n",
        "\n",
        "        # filter training and validation by max_len number of tokens                           \n",
        "        self.data_train = self.data_train.filter(filter_max_length)\n",
        "        self.data_valid = self.data_valid.filter(filter_max_length)\n",
        "\n",
        "        # increase performance by caching training dataset                                     \n",
        "        self.data_train = self.data_train.cache()\n",
        "\n",
        "        # shuffle the training dataset                                                         \n",
        "        data_size = sum(1 for data in self.data_train)\n",
        "        self.data_train = self.data_train.shuffle(data_size)\n",
        "\n",
        "        # split training and validation datasets into padded batches                           \n",
        "        self.data_train = self.data_train.padded_batch(batch_size)\n",
        "        self.data_valid = self.data_valid.padded_batch(batch_size)\n",
        "\n",
        "        # increase performance by prefetching training dataset                                 \n",
        "        self.data_train = self.data_train.prefetch(\n",
        "            tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"                                                                                    \n",
        "        Creates sub_word tokenizers for our dataset                                            \n",
        "                                                                                               \n",
        "        parameters:                                                                            \n",
        "            data [tf.data.Dataset]:                                                            \n",
        "                dataset to use whose examples are formatted as tuple (pt, en)                  \n",
        "                pt [tf.Tensor]:                                                                \n",
        "                    contains the Portuguese sentence                                           \n",
        "                en [tf.Tensor]:                                                                \n",
        "                    contains the corresponding English sentence                                \n",
        "        returns:                                                                               \n",
        "            tokenizer_pt, tokenizer_en:                                                        \n",
        "                tokenizer_pt: the Portuguese tokenizer                                         \n",
        "                tokenizer_en: the English tokenizer                                            \n",
        "        \"\"\"\n",
        "        SubwordTextEncoder = tfds.deprecated.text.SubwordTextEncoder\n",
        "        tokenizer_pt = SubwordTextEncoder.build_from_corpus(\n",
        "            (pt.numpy() for pt, en in data),\n",
        "            target_vocab_size=(2 ** 15))\n",
        "        tokenizer_en = SubwordTextEncoder.build_from_corpus(\n",
        "            (en.numpy() for pt, en in data),\n",
        "            target_vocab_size=(2 ** 15))\n",
        "        return tokenizer_pt, tokenizer_en\n",
        "\n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"                                                                                    \n",
        "        Encodes a translation into tokens                                                      \n",
        "                                                                                               \n",
        "        parameters:                                                                            \n",
        "            pt [tf.Tensor]:                                                                    \n",
        "                contains the Portuguese sentence                                               \n",
        "            en [tf.Tensor]:                                                                    \n",
        "                contains the corresponding English sentence                                    \n",
        "        returns:                                                                               \n",
        "            pt_tokens, en_tokens:                                                              \n",
        "                pt_tokens [np.ndarray]: the Portuguese tokens                                  \n",
        "                en_tokens [np.ndarray]: the English tokens                                     \n",
        "        \"\"\"\n",
        "        pt_start_index = self.tokenizer_pt.vocab_size\n",
        "        pt_end_index = pt_start_index + 1\n",
        "        en_start_index = self.tokenizer_en.vocab_size\n",
        "        en_end_index = en_start_index + 1\n",
        "        pt_tokens = [pt_start_index] + self.tokenizer_pt.encode(\n",
        "            pt.numpy()) + [pt_end_index]\n",
        "        en_tokens = [en_start_index] + self.tokenizer_en.encode(\n",
        "            en.numpy()) + [en_end_index]\n",
        "        return pt_tokens, en_tokens\n",
        "\n",
        "    def tf_encode(self, pt, en):\n",
        "        \"\"\"                                                                                    \n",
        "        Acts as a TensorFlow wrapper for the encode method                                     \n",
        "            to return tensors instead of numpy arrays                                          \n",
        "                                                                                               \n",
        "        parameters:                                                                            \n",
        "            pt [tf.Tensor]:                                                                    \n",
        "                contains the Portuguese sentence                                               \n",
        "            en [tf.Tensor]:                                                                    \n",
        "                contains the corresponding English sentence                                    \n",
        "                                                                                               \n",
        "        returns:                                                                               \n",
        "            pt [tf.Tensor]: encoded Portuguese sentence                                        \n",
        "            en [tf.Tensor]: encoded English sentence                                           \n",
        "        \"\"\"\n",
        "        pt_encoded, en_encoded = tf.py_function(func=self.encode,\n",
        "                                                inp=[pt, en],\n",
        "                                                Tout=[tf.int64, tf.int64])\n",
        "        pt_encoded.set_shape([None])\n",
        "        en_encoded.set_shape([None])\n",
        "        return pt_encoded, en_encoded\n",
        "\n",
        "def create_masks(inputs, target):\n",
        "    \"\"\"                                                                                        \n",
        "    Creates all masks for training/validation                                                  \n",
        "                                                                                               \n",
        "    parameters:                                                                                \n",
        "        inputs [tf.Tensor of shape (batch_size, seq_len_in)]:                                  \n",
        "            contains the input sentence                                                        \n",
        "        target [tf.Tensor of shape (batch_size, seq_len_out)]:                                 \n",
        "            contains the target sentence                                                       \n",
        "    returns:                                                                                   \n",
        "        encoder_mask, combined_mask, decoder_mask                                              \n",
        "        encoder_mask [tf.Tensor of shape                                                       \n",
        "                        (batch_size, 1, 1, seq_len_in)]:                                       \n",
        "            padding mask to be applied to the encoder                                          \n",
        "        combined_mask [tf.Tensor of shape                                                      \n",
        "                        (batch_size, 1, seq_len_out, seq_len_out)]:                            \n",
        "            mask used in the 1st attention block in the decoder to pad                         \n",
        "                and mask future tokens in the input received by the decoder                    \n",
        "            Maximum between look ahead mask and decoder target padding mask                    \n",
        "        decoder_mask [tf.Tensor of shape (batch_size, 1, 1, seq_len_in)]:                      \n",
        "            padding mask used in the 2nd attention block in the decoder                        \n",
        "    \"\"\"\n",
        "    encoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "    encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    decoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "    decoder_mask = decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    batch_size, seq_len_out = target.shape\n",
        "\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones(\n",
        "        (seq_len_out, seq_len_out)), -1, 0)\n",
        "    look_ahead_mask = 1 - look_ahead_mask\n",
        "\n",
        "    padding_mask = tf.cast(tf.math.equal(target, 0), tf.float32)\n",
        "    padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    combined_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "    return encoder_mask, combined_mask, decoder_mask\n",
        "\n",
        "def get_angle(pos, i, dm):\n",
        "    \"\"\"                                                                                         \n",
        "    Calculates the angles for the following formulas for positional encoding:                   \n",
        "                                                                                                \n",
        "    PE(pos, 2i) = sin(pos / 10000^(2i / dm))                                                    \n",
        "    PE(pos, 2i + 1) = cos(pos / 10000^(2i / dm))                                                \n",
        "    \"\"\"\n",
        "    angle_rates = 1 / (10000 ** (i / dm))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(max_seq_len, dm):\n",
        "    \"\"\"                                                                                         \n",
        "    Calculates the positional encoding for a transformer                                        \n",
        "                                                                                                \n",
        "    parameters:                                                                                 \n",
        "        max_seq_len [int]:                                                                      \n",
        "            represents the maximum sequence length                                              \n",
        "        dm: model depth                                                                         \n",
        "                                                                                                \n",
        "    returns:                                                                                    \n",
        "        [numpy.ndarray of shape (max_seq_len, dm)]:                                             \n",
        "            contains the positional encoding vectors                                            \n",
        "    \"\"\"\n",
        "    positional_encoding = np.zeros([max_seq_len, dm])\n",
        "\n",
        "    for pos in range(max_seq_len):\n",
        "        for i in range(0, dm, 2):\n",
        "            # sin for even indices of positional_encoding                                       \n",
        "            positional_encoding[pos, i] = np.sin(get_angle(pos, i, dm))\n",
        "            # cos for odd indices of positional_encoding                                        \n",
        "            positional_encoding[pos, i + 1] = np.cos(get_angle(pos, i, dm))\n",
        "    return positional_encoding\n",
        "\n",
        "def sdp_attention(Q, K, V, mask=None):\n",
        "    \"\"\"                                                                                         \n",
        "    Calculates the scaled dot product attention                                                 \n",
        "                                                                                                \n",
        "    parameters:                                                                                 \n",
        "        Q [tensor with last two dimensions as (..., seq_len_q, dk)]:                            \n",
        "            contains the query matrix                                                           \n",
        "        K [tensor with last two dimensions as (..., seq_len_v, dk)]:                            \n",
        "            contains the key matrix                                                             \n",
        "        V [tensor with last two dimensions as (..., seq_len_v, dv)]:                            \n",
        "            contains the value matrix                                                           \n",
        "        mask [tensor that can be broadcast into (..., seq_len_q, seq_len_v)]:                   \n",
        "            contains the optional mask, or defaulted to None                                    \n",
        "                                                                                                \n",
        "    returns:                                                                                    \n",
        "        outputs, weights:                                                                       \n",
        "            outputs [tensor with last two dimensions as (..., seq_len_q, dv)]:                  \n",
        "                contains the scaled dot product attention                                       \n",
        "            weights [tensor with last two dimensions as                                         \n",
        "                    (..., seq_len_q, seq_len_v)]:                                               \n",
        "                contains the attention weights                                                  \n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(Q, K, transpose_b=True)\n",
        "    # scale matmul_qk                                                                           \n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add mask to scaled tensor                                                                 \n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    # normalize softmax on last axis so all scores add up to 1                                  \n",
        "    weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    # calculate outputs                                                                         \n",
        "    outputs = tf.matmul(weights, V)\n",
        "    return outputs, weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to perform multi-head attention                                                       \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, dm, h)                                                               \n",
        "                                                                                                \n",
        "    public instance attribute:                                                                  \n",
        "        h: number of heads                                                                      \n",
        "        dm: the dimensionality of the model                                                     \n",
        "        depth: the depth of each attention head                                                 \n",
        "        Wq: a Dense layer with dm units, used to generate the query matrix                      \n",
        "        Wk: a Dense layer with dm units, used to generate the key matrix                        \n",
        "        Wv: a Dense layer with dm units, used to generate the value matrix                      \n",
        "        linear: a Dense layer with dm units, used to generate attention output                  \n",
        "                                                                                                \n",
        "    public instance methods:                                                                    \n",
        "        def call(self, Q, K, V, mask):                                                          \n",
        "            generates the query, key, and value matrices and                                    \n",
        "                outputs the scaled dot product attention                                        \n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "\n",
        "        sets the public instance attributes:                                                    \n",
        "            h: number of heads                                                                  \n",
        "            dm: the dimensionality of the model                                                 \n",
        "            depth: the depth of each attention head                                             \n",
        "            Wq: a Dense layer with dm units, used to generate the query matrix                  \n",
        "            Wk: a Dense layer with dm units, used to generate the key matrix                    \n",
        "            Wv: a Dense layer with dm units, used to generate the value matrix                  \n",
        "            linear: a Dense layer with dm units,                                                \n",
        "                used to generate attention output                                               \n",
        "        \"\"\"\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.h = h\n",
        "        self.dm = dm\n",
        "        self.depth = dm // h\n",
        "        self.Wq = tf.keras.layers.Dense(units=dm)\n",
        "        self.Wk = tf.keras.layers.Dense(units=dm)\n",
        "        self.Wv = tf.keras.layers.Dense(units=dm)\n",
        "        self.linear = tf.keras.layers.Dense(units=dm)\n",
        "\n",
        "    def split_heads(self, x, batch):\n",
        "        \"\"\"                                                                                     \n",
        "        Splits the last dimension of tensor into (h, dm) and                                    \n",
        "            transposes the result so the shape is (batch, h, seq_len, dm)                       \n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch, -1, self.h, self.depth))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return x\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"                                                                                     \n",
        "        Generates the query, key, and value matrices and                                        \n",
        "            outputs the scaled dot product attention                                            \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            Q [tensor of shape (batch, seq_len_q, dk)]:                                         \n",
        "                contains the input to generate the query matrix                                 \n",
        "            K [tensor of shape (batch, seq_len_v, dk)]:                                         \n",
        "                contains the input to generate the key matrix                                   \n",
        "            V [tensor of shape (batch, seq_len_v, dv)]:                                         \n",
        "                contains the input to generate the value matrix                                 \n",
        "            mask [always None]                                                                  \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            outputs, weights:                                                                   \n",
        "                outputs [tensor with last two dimensions (..., seq_len_q, dm)]:                 \n",
        "                    contains the scaled dot product attention                                   \n",
        "                weights [tensor with last dimensions                                            \n",
        "                        (..., h, seq_len_q, seq_len_v)]:                                        \n",
        "                    contains the attention weights                                              \n",
        "        \"\"\"\n",
        "        # batch = Q.get_shape().as_list()[0]                                                    \n",
        "        batch = tf.shape(Q)[0]\n",
        "\n",
        "        q = self.Wq(Q)\n",
        "        k = self.Wk(K)\n",
        "        v = self.Wv(V)\n",
        "\n",
        "        q = self.split_heads(q, batch)\n",
        "        k = self.split_heads(k, batch)\n",
        "        v = self.split_heads(v, batch)\n",
        "\n",
        "        attention, weights = sdp_attention(q, k, v, mask)\n",
        "\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch, -1, self.dm))\n",
        "        outputs = self.linear(concat_attention)\n",
        "\n",
        "        return outputs, weights\n",
        "\n",
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to create an encoder block for a transformer                                          \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, dm, h, hidden, drop_rate=0.1)                                        \n",
        "                                                                                                \n",
        "    public instance attribute:                                                                  \n",
        "        mha: MultiHeadAttention layer                                                           \n",
        "        dense_hidden: the hidden dense layer with hidden units, relu activation                 \n",
        "        dense_output: the output dense layer with dm units                                      \n",
        "        layernorm1: the first layer norm layer, with epsilon=1e-6                               \n",
        "        layernorm2: the second layer norm layer, with epsilon=1e-6                              \n",
        "        drouput1: the first dropout layer                                                       \n",
        "        dropout2: the second dropout layer                                                      \n",
        "                                                                                                \n",
        "    public instance method:                                                                     \n",
        "        call(self, x, training, mask=None):                                                     \n",
        "            calls the encoder block and returns the block's output                              \n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "            hidden [int]:                                                                       \n",
        "                represents the number of hidden units in fully connected layer                  \n",
        "            drop_rate [float]:                                                                  \n",
        "                the dropout rate                                    \n",
        "        sets the public instance attributes:                                                    \n",
        "            mha: MultiHeadAttention layer                                                       \n",
        "            dense_hidden: the hidden dense layer with hidden units, relu activ.                 \n",
        "            dense_output: the output dense layer with dm units                                  \n",
        "            layernorm1: the first layer norm layer, with epsilon=1e-6                           \n",
        "            layernorm2: the second layer norm layer, with epsilon=1e-6                          \n",
        "            drouput1: the first dropout layer                                                   \n",
        "            dropout2: the second dropout layer                                                  \n",
        "        \"\"\"\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        if type(hidden) is not int:\n",
        "            raise TypeError(\n",
        "                \"hidden must be int representing number of hidden units\")\n",
        "        if type(drop_rate) is not float:\n",
        "            raise TypeError(\n",
        "                \"drop_rate must be float representing dropout rate\")\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dm, h)\n",
        "        self.dense_hidden = tf.keras.layers.Dense(units=hidden,\n",
        "                                                  activation='relu')\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        \"\"\"                                                                                     \n",
        "        Calls the encoder block and returns the block's output                                  \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            x [tensor of shape (batch, input_seq_len, dm)]:                                     \n",
        "                contains the input to the encoder block                                         \n",
        "            training [boolean]:                                                                 \n",
        "                determines if the model is in training                                          \n",
        "            mask:                                                                               \n",
        "                mask to be applied for multi-head attention                                     \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            [tensor of shape (batch, input_seq_len, dm)]:                                       \n",
        "                contains the block's output                                                     \n",
        "        \"\"\"\n",
        "        attention_output, _ = self.mha(x, x, x, mask)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        output1 = self.layernorm1(x + attention_output)\n",
        "\n",
        "        dense_output = self.dense_hidden(output1)\n",
        "        ffn_output = self.dense_output(dense_output)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        output2 = self.layernorm2(output1 + ffn_output)\n",
        "\n",
        "        return output2\n",
        "\n",
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to create a decoder block for a transformer                                           \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, dm, h, hidden, drop_rate=0.1)                                        \n",
        "                                                                                                \n",
        "    public instance attribute:                                                                  \n",
        "        mha1: the first MultiHeadAttention layer                                                \n",
        "        mha2: the second MultiHeadAttention layer                                               \n",
        "        dense_hidden: the hidden dense layer with hidden units, relu activation                 \n",
        "        dense_output: the output dense layer with dm units                                      \n",
        "        layernorm1: the first layer norm layer, with epsilon=1e-6                               \n",
        "        layernorm2: the second layer norm layer, with epsilon=1e-6                              \n",
        "        layernorm3: the third layer norm layer, with epsilon=1e-6                               \n",
        "        drouput1: the first dropout layer                                                       \n",
        "        dropout2: the second dropout layer                                                      \n",
        "        dropout3: the third dropout layer                                                       \n",
        "                                                                                                \n",
        "    public instance method:                                                                     \n",
        "        def call(self, x, encoder_output, training, look_ahead_mask,                            \n",
        "                    padding_mask):                                                              \n",
        "            calls the decoder block and returns the block's output                              \n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "            hidden [int]:                                                                       \n",
        "                represents the number of hidden units in fully connected layer                  \n",
        "            drop_rate [float]:                                                                  \n",
        "                the dropout rate                                                                \n",
        "                                                                          \n",
        "        sets the public instance attributes:                                                    \n",
        "            mha1: the first MultiHeadAttention layer                                            \n",
        "            mha2: the second MultiHeadAttention layer                                           \n",
        "            dense_hidden: the hidden dense layer with hidden units, relu activ.                 \n",
        "            dense_output: the output dense layer with dm units                                  \n",
        "            layernorm1: the first layer norm layer, with epsilon=1e-6                           \n",
        "            layernorm2: the second layer norm layer, with epsilon=1e-6                          \n",
        "            layernorm3: the third layer norm layer, with epsilon=1e-6                           \n",
        "            drouput1: the first dropout layer                                                   \n",
        "            dropout2: the second dropout layer                                                  \n",
        "            dropout3: the third dropout layer                                                   \n",
        "        \"\"\"\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        if type(hidden) is not int:\n",
        "            raise TypeError(\n",
        "                \"hidden must be int representing number of hidden units\")\n",
        "        if type(drop_rate) is not float:\n",
        "            raise TypeError(\n",
        "                \"drop_rate must be float representing dropout rate\")\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(dm, h)\n",
        "        self.mha2 = MultiHeadAttention(dm, h)\n",
        "        self.dense_hidden = tf.keras.layers.Dense(units=hidden,\n",
        "                                                  activation='relu')\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"                                                                                     \n",
        "        Calls the decoder block and returns the block's output                                  \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            x [tensor of shape (batch, target_seq_len, dm)]:                                    \n",
        "                contains the input to the decoder block                                         \n",
        "            encoder_output [tensor of shape (batch, input_seq_len, dm)]:                        \n",
        "                contains the output of the encoder                                              \n",
        "            training [boolean]:                                                                 \n",
        "                determines if the model is in training                                          \n",
        "            look_ahead_mask:                                                                    \n",
        "                mask to be applied to the first multi-head attention                            \n",
        "            padding_mask:                                                                       \n",
        "                mask to be applied to the second multi-head attention                           \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            [tensor of shape (batch, target_seq_len, dm)]:                                      \n",
        "                contains the block's output                                                     \n",
        "        \"\"\"\n",
        "        attention_output1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attention_output1 = self.dropout1(attention_output1, training=training)\n",
        "        output1 = self.layernorm1(x + attention_output1)\n",
        "\n",
        "        attention_output2, _ = self.mha2(output1, encoder_output,\n",
        "                                         encoder_output, padding_mask)\n",
        "        attention_output2 = self.dropout2(attention_output2, training=training)\n",
        "        output2 = self.layernorm2(output1 + attention_output2)\n",
        "\n",
        "        dense_output = self.dense_hidden(output2)\n",
        "        ffn_output = self.dense_output(dense_output)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        output3 = self.layernorm3(output2 + ffn_output)\n",
        "\n",
        "        return output3\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to create the encoder for a transformer                                               \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, N, dm, h, hidden, input_vocab, max_seq_len,                          \n",
        "                        drop_rate=0.1)                                                          \n",
        "                                                                                                \n",
        "    public instance attribute:                                                                  \n",
        "        N: the number of blocks in the encoder                                                  \n",
        "        dm: the dimensionality of the model                                                     \n",
        "        embedding: the embedding layer for the inputs                                           \n",
        "        positional_encoding [numpy.ndarray of shape (max_seq_len, dm)]:                         \n",
        "            contains the positional encodings                                                   \n",
        "        blocks [list of length N]:                                                              \n",
        "            contains all the EncoderBlocks                                                      \n",
        "        dropout: the dropout layer, to be applied to the positional encodings                   \n",
        "                                                                                                \n",
        "    public instance method:                                                                     \n",
        "        call(self, x, training, mask):                                                          \n",
        "            calls the encoder and returns the encoder's output                                  \n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, input_vocab, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            N [int]:                                                                            \n",
        "                represents the number of blocks in the encoder                                  \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "            hidden [int]:                                                                       \n",
        "                represents the number of hidden units in fully connected layer                  \n",
        "            input_vocab [int]:                                                                  \n",
        "                represents the size of the input vocabulary             \n",
        "            max_seq_len [int]:                                                                  \n",
        "                represents the maximum sequence length possible                                 \n",
        "            drop_rate [float]:                                                                  \n",
        "                the dropout rate                                                                \n",
        "                                                                                                \n",
        "        sets the public instance attributes:                                                    \n",
        "            N: the number of blocks in the encoder                                              \n",
        "            dm: the dimensionality of the model                                                 \n",
        "            embedding: the embedding layer for the inputs                                       \n",
        "            positional_encoding [numpy.ndarray of shape (max_seq_len, dm)]:                     \n",
        "                contains the positional encodings                                               \n",
        "            blocks [list of length N]:                                                          \n",
        "                contains all the EncoderBlocks                                                  \n",
        "            dropout: the dropout layer, applied to the positional encodings                     \n",
        "        \"\"\"\n",
        "        if type(N) is not int:\n",
        "            raise TypeError(\n",
        "                \"N must be int representing number of blocks in the encoder\")\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        if type(hidden) is not int:\n",
        "            raise TypeError(\n",
        "                \"hidden must be int representing number of hidden units\")\n",
        "        if type(input_vocab) is not int:\n",
        "            raise TypeError(\n",
        "                \"input_vocab must be int representing size of input vocab\")\n",
        "        if type(max_seq_len) is not int:\n",
        "            raise TypeError(\n",
        "                \"max_seq_len must be int representing max sequence length\")\n",
        "        if type(drop_rate) is not float:\n",
        "            raise TypeError(\n",
        "                \"drop_rate must be float representing dropout rate\")\n",
        "        super(Encoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.dm = dm\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=input_vocab,\n",
        "                                                   output_dim=dm)\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        self.blocks = [EncoderBlock(dm, h, hidden, drop_rate)\n",
        "                       for block in range(N)]\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"                                                                                     \n",
        "        Calls the encoder and returns the encoder's output                                      \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            x [tensor of shape (batch, input_seq_len, dm)]:                                     \n",
        "                contains the input to the encoder                                               \n",
        "            training [boolean]:                                                                 \n",
        "                determines if the model is in training                                          \n",
        "            mask:                                                                               \n",
        "                mask to be applied for multi-head attention                                     \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            [tensor of shape (batch, input_seq_len, dm)]:                                       \n",
        "                contains the encoder output                                                     \n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.dm, tf.float32))\n",
        "        x += self.positional_encoding[:seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.blocks[i](x, training, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to create the decoder for a transformer                                               \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, N, dm, h, hidden, target_vocab, max_seq_len,                         \n",
        "                        drop_rate=0.1)                                                          \n",
        "                                                                                                \n",
        "    public instance attribute:                                                                  \n",
        "        N: the number of blocks in the encoder                                                  \n",
        "        dm: the dimensionality of the model                                                     \n",
        "        embedding: the embedding layer for the targets                                          \n",
        "        positional_encoding [numpy.ndarray of shape (max_seq_len, dm)]:                         \n",
        "            contains the positional encodings                                                   \n",
        "        blocks [list of length N]:                                                              \n",
        "            contains all the DecoderBlocks                                                      \n",
        "        dropout: the dropout layer, to be applied to the positional encodings                   \n",
        "                                                                                                \n",
        "    public instance method:                                                                     \n",
        "        def call(self, x, encoder_output, training, look_ahead_mask,                            \n",
        "                    padding_mask):                                                              \n",
        "            calls the decoder and returns the decoder's output                                  \n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, target_vocab, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            N [int]:                                                                            \n",
        "                represents the number of blocks in the encoder                                  \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "            hidden [int]:                                                                       \n",
        "                represents the number of hidden units in fully connected layer                  \n",
        "            target_vocab [int]:                                                                 \n",
        "                represents the size of the target vocabulary      \n",
        "            max_seq_len [int]:                                                                  \n",
        "                represents the maximum sequence length possible                                 \n",
        "            drop_rate [float]:                                                                  \n",
        "                the dropout rate                                                                \n",
        "                                                                                                \n",
        "        sets the public instance attributes:                                                    \n",
        "            N: the number of blocks in the encoder                                              \n",
        "            dm: the dimensionality of the model                                                 \n",
        "            embedding: the embedding layer for the targets                                      \n",
        "            positional_encoding [numpy.ndarray of shape (max_seq_len, dm)]:                     \n",
        "                contains the positional encodings                                               \n",
        "            blocks [list of length N]:                                                          \n",
        "                contains all the DecoderBlocks                                                  \n",
        "            dropout: the dropout layer,                                                         \n",
        "                to be applied to the positional encodings                                       \n",
        "        \"\"\"\n",
        "        if type(N) is not int:\n",
        "            raise TypeError(\n",
        "                \"N must be int representing number of blocks in the encoder\")\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        if type(hidden) is not int:\n",
        "            raise TypeError(\n",
        "                \"hidden must be int representing number of hidden units\")\n",
        "        if type(target_vocab) is not int:\n",
        "            raise TypeError(\n",
        "                \"target_vocab must be int representing size of target vocab\")\n",
        "        if type(max_seq_len) is not int:\n",
        "            raise TypeError(\n",
        "                \"max_seq_len must be int representing max sequence length\")\n",
        "        if type(drop_rate) is not float:\n",
        "            raise TypeError(\n",
        "                \"drop_rate must be float representing dropout rate\")\n",
        "        super(Decoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.dm = dm\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=target_vocab,\n",
        "                                                   output_dim=dm)\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        self.blocks = [DecoderBlock(dm, h, hidden, drop_rate)\n",
        "                       for block in range(N)]\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"                                                                                     \n",
        "        Calls the decoder and returns the decoder's output                                      \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            x [tensor of shape (batch, target_seq_len, dm)]:                                    \n",
        "                contains the input to the decoder                                               \n",
        "            encoder_output [tensor of shape (batch, input_seq_len, dm)]:                        \n",
        "                contains the output of the encoder                                              \n",
        "            training [boolean]:                                                                 \n",
        "                determines if the model is in training                                          \n",
        "            look_ahead_mask:                                                                    \n",
        "                mask to be applied to first multi-head attention                                \n",
        "            padding_mask:                                                                       \n",
        "                mask to be applied to second multi-head attention                               \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            [tensor of shape (batch, target_seq_len, dm)]:                                      \n",
        "                contains the decoder output                                                     \n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.dm, tf.float32))\n",
        "        x += self.positional_encoding[:seq_len]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.blocks[i](x, encoder_output, training,\n",
        "                               look_ahead_mask, padding_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "    \"\"\"                                                                                         \n",
        "    Class to create the transformer network                                                     \n",
        "                                                                                                \n",
        "    class constructor:                                                                          \n",
        "        def __init__(self, N, dm, h, hidden, input_vocab, target_vocab,                         \n",
        "                     max_seq_input, max_seq_target, drop_rate=0.1)                              \n",
        "                                                                                                \n",
        "    public instance attributes:                                                                 \n",
        "        encoder: the encoder layer                                                              \n",
        "        decoder: the decoder layer                                                              \n",
        "        linear: the Dense layer with target_vocab units                                         \n",
        "                                                                                                \n",
        "    public instance method:                                                                     \n",
        "        def call(self, inputs, target, training, encoder_mask,                                  \n",
        "                    look_ahead_mask, decoder_mask):                                             \n",
        "            calls the transformer network and returns the transformer output                    \n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, input_vocab, target_vocab,\n",
        "                 max_seq_input, max_seq_target, drop_rate=0.1):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            N [int]:                                                                            \n",
        "                represents the number of blocks in the encoder and decoder                      \n",
        "            dm [int]:                                                                           \n",
        "                represents the dimensionality of the model                                      \n",
        "            h [int]:                                                                            \n",
        "                represents the number of heads                                                  \n",
        "            hidden [int]:                                                                       \n",
        "                represents the number of hidden units in fully connected layer                  \n",
        "            input_vocab [int]:                                                                  \n",
        "                represents the size of the input vocabulary                       \n",
        "            target_vocab [int]:                                                                 \n",
        "                represents the size of the target vocabulary                                    \n",
        "            max_seq_input [int]:                                                                \n",
        "                represents the maximum sequence length possible for input                       \n",
        "            max_seq_target [int]:                                                               \n",
        "                represents the maximum sequence length possible for target                      \n",
        "            drop_rate [float]:                                                                  \n",
        "                the dropout rate                                                                \n",
        "                                                                                                \n",
        "        sets the public instance attributes:                                                    \n",
        "            encoder: the encoder layer                                                          \n",
        "            decoder: the decoder layer                                                          \n",
        "            linear: the Dense layer with target_vocab units                                     \n",
        "        \"\"\"\n",
        "        if type(N) is not int:\n",
        "            raise TypeError(\n",
        "                \"N must be int representing number of blocks in the encoder\")\n",
        "        if type(dm) is not int:\n",
        "            raise TypeError(\n",
        "                \"dm must be int representing dimensionality of model\")\n",
        "        if type(h) is not int:\n",
        "            raise TypeError(\n",
        "                \"h must be int representing number of heads\")\n",
        "        if type(hidden) is not int:\n",
        "            raise TypeError(\n",
        "                \"hidden must be int representing number of hidden units\")\n",
        "        if type(input_vocab) is not int:\n",
        "            raise TypeError(\n",
        "                \"input_vocab must be int representing size of input vocab\")\n",
        "        if type(target_vocab) is not int:\n",
        "            raise TypeError(\n",
        "                \"target_vocab must be int representing size of target vocab\")\n",
        "        if type(max_seq_input) is not int:\n",
        "            raise TypeError(\n",
        "                \"max_seq_input must be int representing max length for input\")\n",
        "        if type(max_seq_target) is not int:\n",
        "            raise TypeError(\n",
        "                \"max_seq_target must be int representing max len for target\")\n",
        "        if type(drop_rate) is not float:\n",
        "            raise TypeError(\n",
        "                \"drop_rate must be float representing dropout rate\")\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            N, dm, h, hidden, input_vocab, max_seq_input, drop_rate)\n",
        "        self.decoder = Decoder(\n",
        "            N, dm, h, hidden, target_vocab, max_seq_target, drop_rate)\n",
        "        self.linear = tf.keras.layers.Dense(units=target_vocab)\n",
        "\n",
        "    def call(self, inputs, target, training, encoder_mask, look_ahead_mask,\n",
        "             decoder_mask):\n",
        "        \"\"\"                                                                                     \n",
        "        Calls the transformer network and returns the transformer output                        \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            inputs [tensor of shape (batch, input_seq_len)]:                                    \n",
        "                contains the inputs                                                             \n",
        "            target [tensor of shape (batch, target_seq_len)]:                                   \n",
        "                contains the target                                                             \n",
        "            training [boolean]:                                                                 \n",
        "                determines if the model is in training                                          \n",
        "            encoder_mask:                                                                       \n",
        "                padding mask to be applied to the encoder                                       \n",
        "            look_ahead_mask:                                                                    \n",
        "                look ahead mask to be applied to the decoder                                    \n",
        "            decoder_mask:                                                                       \n",
        "                padding mask to be applied to the decoder                                       \n",
        "                                                                                                \n",
        "        returns:                                                                                \n",
        "            [tensor of shape (batch, target_seq_len, target_vocab)]:                            \n",
        "                contains the transformer output                                                 \n",
        "        \"\"\"\n",
        "        encoder_output = self.encoder(inputs, training, encoder_mask)\n",
        "        decoder_output = self.decoder(target, encoder_output, training,\n",
        "                                      look_ahead_mask, decoder_mask)\n",
        "        final_output = self.linear(decoder_output)\n",
        "        return final_output\n",
        "\n",
        "class LearningSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"                                                                                         \n",
        "    Establishes learning rate schedule for training transformer model                           \n",
        "                                                                                                \n",
        "    Utilizes given function for learning rate:                                                  \n",
        "    l_rate = (dm ** -0.5) * min(                                                                \n",
        "                     (step_num ** -0.5), (step_num * warmup_steps ** -1.5))                     \n",
        "    \"\"\"\n",
        "    def __init__(self, dm, warmup_steps=4000):\n",
        "        \"\"\"                                                                                     \n",
        "        Class constructor                                                                       \n",
        "                                                                                                \n",
        "        parameters:                                                                             \n",
        "            dm [int]:                                                                           \n",
        "                dimensionality of the model                                                     \n",
        "            warmup_steps [int]:                                                                 \n",
        "                number of warmup steps, default=4000                                            \n",
        "        \"\"\"\n",
        "        super(LearningSchedule, self).__init__()\n",
        "\n",
        "        self.dm = tf.cast(dm, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"                                                                                     \n",
        "        Evaluates the learning rate for the given step                                          \n",
        "        \"\"\"\n",
        "        rsqrt_dm = tf.math.rsqrt(self.dm)\n",
        "        rsqrt_step_arg = tf.math.rsqrt(step)\n",
        "        warmup_step_arg = step * (self.warmup_steps ** -1.5)\n",
        "        l_rate = rsqrt_dm * tf.math.minimum(rsqrt_step_arg, warmup_step_arg)\n",
        "        return l_rate\n",
        "\n",
        "def train_transformer(N, dm, h, hidden, max_len, batch_size, epochs):\n",
        "    \"\"\"                                                                                         \n",
        "    Creates and trains a transformer for machine translation                                    \n",
        "        of Portuguese to English using previously created dataset                               \n",
        "                                                                                                \n",
        "    parameters:                                                                                 \n",
        "        N [int]:                                                                                \n",
        "            number of blocks in the encoder and decoder                                         \n",
        "        dm [int]:                                                                               \n",
        "            dimensionality of the model                                                         \n",
        "        h [int]:                                                                                \n",
        "            number of heads                                                                     \n",
        "        hidden [int]:                                                                           \n",
        "            number of hidden units in the fully connected layers                                \n",
        "        max_len [int]:                                                                          \n",
        "            maximum number of tokens per sequence                                               \n",
        "        batch_size [int]:                                                                       \n",
        "            batch size used for training                                                        \n",
        "        epochs [int]:                                                                           \n",
        "            number of epochs to train for                                                       \n",
        "    returns:                                                                                    \n",
        "        the trained model                                                                       \n",
        "    \"\"\"\n",
        "    data = Dataset(batch_size, max_len)\n",
        "    input_vocab = data.tokenizer_pt.vocab_size + 2\n",
        "    target_vocab = data.tokenizer_en.vocab_size + 2\n",
        "    # encoder = data.tokenizer_pt                                                               \n",
        "    # decoder = data.tokenizer_en                                                               \n",
        "\n",
        "    transformer = Transformer(N, dm, h, hidden,\n",
        "                              input_vocab, target_vocab,\n",
        "                              max_len, max_len)\n",
        "\n",
        "    learning_rate = LearningSchedule(dm)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                         beta_1=0.9,\n",
        "                                         beta_2=0.98,\n",
        "                                         epsilon=1e-9)\n",
        "    losses = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                           reduction='none')\n",
        "\n",
        "    def loss_function(actual, prediction):\n",
        "        \"\"\"                                                                                     \n",
        "        Calculate the loss from actual value and the prediction                                 \n",
        "        \"\"\"\n",
        "        mask = tf.math.logical_not(tf.math.equal(actual, 0))\n",
        "        loss = losses(actual, prediction)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def accuracy_function(actual, prediction):\n",
        "        \"\"\"                                                                                     \n",
        "        Calculates the accuracy of the prediction                                               \n",
        "        \"\"\"\n",
        "        accuracies = tf.equal(actual, tf.argmax(prediction, axis=2))\n",
        "        mask = tf.math.logical_not(tf.math.equal(actual, 0))\n",
        "        accuracies = tf.math.logical_and(mask, accuracies)\n",
        "        accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch = 0\n",
        "        for (input, target) in data.data_train:\n",
        "            target_input = target[:, :-1]\n",
        "            target_actual = target[:, 1:]\n",
        "            encoder_mask, look_ahead_mask, decoder_mask = create_masks(\n",
        "                input, target_input)\n",
        "            with tf.GradientTape() as tape:\n",
        "                prediction = transformer(input, target_input, True,\n",
        "                                         encoder_mask, look_ahead_mask,\n",
        "                                         decoder_mask)\n",
        "                loss = loss_function(target_actual, prediction)\n",
        "            gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(\n",
        "                gradients, transformer.trainable_variables))\n",
        "            t_loss = train_loss(loss)\n",
        "            t_accuracy = train_accuracy(target_actual, prediction)\n",
        "            if batch % 50 is 0:\n",
        "                print(\"Epoch {}, batch {}: loss {} accuracy {}\".format(\n",
        "                    epoch, batch, t_loss, t_accuracy))\n",
        "            batch += 1\n",
        "        print(\"Epoch {}: loss {} accuracy {}\".format(\n",
        "            epoch, t_loss, t_accuracy))\n",
        "    return transformer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = Dataset()\n",
        "#print(\"TASK 0:\")\n",
        "#for pt, en in data.data_train.take(1):\n",
        "    #print(pt.numpy().decode('utf-8'))\n",
        "    #rint(en.numpy().decode('utf-8'))\n",
        "#for pt, en in data.data_valid.take(1):\n",
        "    #print(pt.numpy().decode('utf-8'))\n",
        "    #print(en.numpy().decode('utf-8'))\n",
        "#print(type(data.tokenizer_pt))\n",
        "#print(type(data.tokenizer_en))\n",
        "#print(\"TASK 1:\")\n",
        "#for pt, en in data.data_train.take(1):\n",
        "    #print(data.encode(pt, en))\n",
        "#for pt, en in data.data_valid.take(1):\n",
        "    #print(data.encode(pt, en))\n",
        "#print(\"TASK 2:\")\n",
        "#for pt, en in data.data_train.take(1):\n",
        "    #print(pt, en)\n",
        "#for pt, en in data.data_valid.take(1):\n",
        "    #print(pt, en)\n",
        "#print(\"TASK 3:\")\n",
        "#tf.compat.v1.set_random_seed(0)\n",
        "#data = Dataset(32, 40)\n",
        "#for pt, en in data.data_train.take(1):\n",
        "    #print(pt, en)\n",
        "#for pt, en in data.data_valid.take(1):\n",
        "    #print(pt, en)\n",
        "#print(\"TASK 4:\")\n",
        "#tf.compat.v1.set_random_seed(0)\n",
        "#data = Dataset(32, 40)\n",
        "#for inputs, target in data.data_train.take(1):\n",
        "    #print(create_masks(inputs, target))\n",
        "print(\"TASK 5:\")\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "transformer = train_transformer(4, 128, 8, 512, 32, 40, 2)\n",
        "print(type(transformer))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TASK 5:\n",
            "Epoch 0, batch 0: loss 10.247076034545898 accuracy 0.0\n",
            "Epoch 0, batch 50: loss 10.212627410888672 accuracy 0.0012723066611215472\n",
            "Epoch 0, batch 100: loss 10.133011817932129 accuracy 0.011149213649332523\n",
            "Epoch 0, batch 150: loss 10.015902519226074 accuracy 0.015223746187984943\n",
            "Epoch 0, batch 200: loss 9.859915733337402 accuracy 0.01601114124059677\n",
            "Epoch 0, batch 250: loss 9.667801856994629 accuracy 0.018717065453529358\n",
            "Epoch 0, batch 300: loss 9.447187423706055 accuracy 0.02274213545024395\n",
            "Epoch 0, batch 350: loss 9.203826904296875 accuracy 0.026899071410298347\n",
            "Epoch 0, batch 400: loss 8.95121955871582 accuracy 0.03125317767262459\n",
            "Epoch 0, batch 450: loss 8.706740379333496 accuracy 0.0348038487136364\n",
            "Epoch 0, batch 500: loss 8.490663528442383 accuracy 0.03764767944812775\n",
            "Epoch 0, batch 550: loss 8.30311393737793 accuracy 0.04032387584447861\n",
            "Epoch 0, batch 600: loss 8.133316040039062 accuracy 0.04309402406215668\n",
            "Epoch 0, batch 650: loss 7.976280689239502 accuracy 0.0464952178299427\n",
            "Epoch 0, batch 700: loss 7.828597068786621 accuracy 0.05041009560227394\n",
            "Epoch 0, batch 750: loss 7.6906514167785645 accuracy 0.05428328737616539\n",
            "Epoch 0, batch 800: loss 7.565177917480469 accuracy 0.057849958539009094\n",
            "Epoch 0, batch 850: loss 7.446243762969971 accuracy 0.061390407383441925\n",
            "Epoch 0, batch 900: loss 7.337075710296631 accuracy 0.06491079181432724\n",
            "Epoch 0, batch 950: loss 7.234829902648926 accuracy 0.06824851036071777\n",
            "Epoch 0, batch 1000: loss 7.1399455070495605 accuracy 0.0713140219449997\n",
            "Epoch 0, batch 1050: loss 7.051123142242432 accuracy 0.07440776377916336\n",
            "Epoch 0: loss 7.012202739715576 accuracy 0.07576613128185272\n",
            "Epoch 1, batch 0: loss 7.010329246520996 accuracy 0.07583340257406235\n",
            "Epoch 1, batch 50: loss 6.926628112792969 accuracy 0.07878666371107101\n",
            "Epoch 1, batch 100: loss 6.848325252532959 accuracy 0.0815785825252533\n",
            "Epoch 1, batch 150: loss 6.771914958953857 accuracy 0.0842449963092804\n",
            "Epoch 1, batch 200: loss 6.7021942138671875 accuracy 0.08675261586904526\n",
            "Epoch 1, batch 250: loss 6.637738227844238 accuracy 0.08920709788799286\n",
            "Epoch 1, batch 300: loss 6.57562780380249 accuracy 0.09151789546012878\n",
            "Epoch 1, batch 350: loss 6.517136096954346 accuracy 0.0937289372086525\n",
            "Epoch 1, batch 400: loss 6.460721492767334 accuracy 0.09584809839725494\n",
            "Epoch 1, batch 450: loss 6.408619403839111 accuracy 0.09783361107110977\n",
            "Epoch 1, batch 500: loss 6.358979225158691 accuracy 0.09982722997665405\n",
            "Epoch 1, batch 550: loss 6.31189489364624 accuracy 0.10165812820196152\n",
            "Epoch 1, batch 600: loss 6.266439914703369 accuracy 0.10351214557886124\n",
            "Epoch 1, batch 650: loss 6.222133636474609 accuracy 0.10529954731464386\n",
            "Epoch 1, batch 700: loss 6.180788993835449 accuracy 0.10693389177322388\n",
            "Epoch 1, batch 750: loss 6.139860153198242 accuracy 0.1086190789937973\n",
            "Epoch 1, batch 800: loss 6.101782321929932 accuracy 0.11020529270172119\n",
            "Epoch 1, batch 850: loss 6.064748764038086 accuracy 0.11178107559680939\n",
            "Epoch 1, batch 900: loss 6.029900074005127 accuracy 0.11325107514858246\n",
            "Epoch 1, batch 950: loss 5.995944023132324 accuracy 0.11472512781620026\n",
            "Epoch 1, batch 1000: loss 5.9636993408203125 accuracy 0.11603378504514694\n",
            "Epoch 1, batch 1050: loss 5.93208646774292 accuracy 0.11739685386419296\n",
            "Epoch 1: loss 5.918064594268799 accuracy 0.11802295595407486\n",
            "<class '__main__.Transformer'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}